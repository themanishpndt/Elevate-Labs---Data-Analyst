# ðŸ“Š Data Analyst Internship - Task 1: Data Cleaning & Preprocessing

## ðŸŽ¯ Objective
The goal of this task is to **clean and preprocess a raw dataset** containing common real-world issues such as:
- Missing values
- Duplicate records
- Inconsistent formatting (text, dates, numeric values)
- Outliers and extreme values
- Invalid data entries (e.g., malformed emails, unrealistic ages)

This ensures the dataset is **accurate, consistent, and ready for analysis or modeling**.

---

## ðŸ“‚ Repository Structure
```
DataAnalyst_Task1_ElevateLabs/
â”œâ”€â”€ raw_dataset.csv              # Original noisy dataset with issues (300 records)
â”œâ”€â”€ cleaned_dataset.csv          # Final cleaned dataset ready for analysis (255 records)
â”œâ”€â”€ task1_cleaning.py            # Python script for automated cleaning pipeline
â”œâ”€â”€ task1_cleaning.ipynb         # Jupyter Notebook (step-by-step explanation + profiling)
â”œâ”€â”€ report.md                    # Detailed cleaning metrics and results
â”œâ”€â”€ data_dictionary.md           # Comprehensive field descriptions
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ Images/                      # Charts & visualizations of data before/after cleaning
â”‚   â”œâ”€â”€ age_hist_raw.png
â”‚   â”œâ”€â”€ age_hist_cleaned.png
â”‚   â”œâ”€â”€ income_boxplot_raw.png
â”‚   â”œâ”€â”€ income_boxplot_cleaned.png
â”‚   â”œâ”€â”€ gender_counts_raw.png
â”‚   â””â”€â”€ gender_counts_cleaned.png
â”œâ”€â”€ LICENSE                      # Open-source license (MIT)
â””â”€â”€ README.md                    # Project documentation (this file)
```

---

## ðŸ›  Tools & Technologies Used
- **Python 3.x** â€“ Data cleaning and preprocessing
- **Pandas** â€“ Data handling and transformations
- **NumPy** â€“ Numerical computations
- **Matplotlib/Seaborn** â€“ Data visualization (before/after comparisons)
- **Jupyter Notebook** â€“ Interactive analysis and documentation

---

## ðŸ“Š Dataset Information
**Source**: Customer Segmentation Dataset (Mall Customer Data style)

### Dataset Columns Handled:
- **CustomerID** - Unique customer identifier
- **Gender** - Customer gender (with inconsistencies)
- **Age** - Customer age (missing values and outliers)
- **Annual Income (k$)** - Income with various formats and symbols
- **Spending Score (1-100)** - Customer spending metric
- **JoinDate** - Multiple date formats
- **Country** - Inconsistent country naming
- **Email** - Validation and cleaning
- **PurchaseCount** - Transaction history
- **LastPurchaseDate** - Date standardization
- **Churn** - Customer retention status

---

## ðŸ§¹ Data Cleaning Steps Performed

### 1. **Handling Missing Values**
- Identified using `.isnull().sum()`
- Filled `Age` with **median**, `Annual Income` with **mean**, `Spending Score` with **median**
- Missing `JoinDate` and `LastPurchaseDate` were filled with the **mode or logical replacements**
- **Categorical columns**: Filled with mode (Gender, Country)
- **Email validation**: Separate flag for invalid emails

### 2. **Removing Duplicates**
- Removed using `.drop_duplicates()`
- Removed 45 exact duplicate records
- Ensured unique records per `CustomerID`
- Maintained data integrity while eliminating redundancy

### 3. **Standardizing Text Data**
- **Gender** standardized â†’ `Male`, `Female`, `Unknown`
- **Country** standardized â†’ e.g., `"usa"`, `"U.S.A"`, `"us"` â†’ `United States`
- **Email** validated for proper format (`@` and domain)
- **Column Headers**: Converted to snake_case format
- **Text Data**: Stripped whitespace and standardized casing

### 4. **Fixing Data Types**
- Converted `Age` to **integer**
- Converted `JoinDate` and `LastPurchaseDate` to **datetime**
- Converted categorical fields (`Gender`, `Churn`) to consistent formats
- `Annual Income` â†’ Float with currency symbol removal
- `Spending Score` â†’ Integer with range validation

### 5. **Outlier Treatment**
- Identified outliers using **IQR method** on `Annual Income` and other numerical columns
- Applied **capping** to reduce the influence of extreme values
- Treated invalid entries (negative ages, unrealistic values)
- Age range constrained to realistic values (10-100)

### 6. **Column Name Standardization**
- Converted to **lowercase**
- Replaced spaces & special characters with underscores (`annual_income_k`)
- Ensured consistent naming convention throughout dataset

### 7. **Data Validation & Business Logic**
- **Email format validation** with proper checking
- **Date consistency validation** (last purchase â‰¥ join date)
- **Churn flag standardization** (Y/Yes/True/1 â†’ 1, others â†’ 0)

---

## ðŸ“Š Results & Performance Metrics

### **Data Quality Before Cleaning:**
- **Total Records**: 300
- **Missing Values**: 47 entries across columns
- **Duplicate Records**: 45
- **Inconsistent Formats**: 89 entries
- **Data Quality Score**: 68%

### **Data Quality After Cleaning:**
- **Total Records**: 255 (clean, unique records)
- **Missing Values**: 0 (completely resolved)
- **Duplicate Records**: 0 (100% removed)
- **Consistent Formats**: 100% achieved
- **Data Quality Score**: 98%

### **Key Improvements Achieved:**
1. **100% Data Integrity** - No missing values or duplicates
2. **Format Consistency** - Uniform data formats across all columns
3. **Business Readiness** - Dataset ready for analysis and modeling
4. **Automated Pipeline** - Reproducible cleaning process

---

## ðŸ“¸ Visual Insights (Before vs After Cleaning)
- **Age Distribution**: Unrealistic values (e.g., 5, 150) replaced with valid ages
- **Annual Income**: Outliers capped, missing values filled
- **Gender Counts**: Standardized across multiple inconsistent labels
- **Country Field**: Normalized to consistent country names

*All visuals are available in the `/Images` folder*

---

## ðŸš€ How to Run This Project

### **Prerequisites**
- Python 3.7+
- pip package manager

### **Installation & Execution**

1. **Clone the repository**:
```bash
git clone https://github.com/yourusername/DataAnalyst_Task1_ElevateLabs.git
cd DataAnalyst_Task1_ElevateLabs
```

2. **Install dependencies**:
```bash
pip install -r requirements.txt
```

3. **Run the cleaning script**:
```bash
python task1_cleaning.py
```
*This will generate `cleaned_dataset.csv`*

4. **Explore the notebook**:
```bash
jupyter notebook task1_cleaning.ipynb
```

### **Quick Start Alternative**
1. **Install dependencies**:
```bash
pip install -r requirements.txt
```

2. **Run the cleaning pipeline**:
```bash
python task1_cleaning.py
```

3. **Explore the results**:
- Check `cleaned_dataset.csv` for the final cleaned data
- Review `report.md` for detailed cleaning metrics
- View `Images/` folder for visual comparisons

---

## ðŸ“œ Deliverables
- âœ… `raw_dataset.csv` â†’ original messy dataset (300 records)
- âœ… `cleaned_dataset.csv` â†’ final processed dataset (255 records)
- âœ… `task1_cleaning.py` â†’ reproducible cleaning pipeline
- âœ… `task1_cleaning.ipynb` â†’ notebook with explanation + profiling
- âœ… `report.md` â†’ summary of issues and fixes
- âœ… `data_dictionary.md` â†’ detailed field descriptions
- âœ… Visual comparisons â†’ before/after cleaning in Images folder

---

## ðŸ“˜ Interview Prep Questions (with Answers)

### 1. **What are missing values and how do you handle them?**
**Answer**: Missing values are blanks or nulls in a dataset. They can be handled by:
- Removing rows/columns with `dropna()`
- Filling with statistical measures using `fillna()` (mean/median/mode)
- Predictive imputation for advanced cases

### 2. **How do you treat duplicate records?**
**Answer**: Use `.drop_duplicates()` in Pandas or "Remove Duplicates" in Excel. Keep the first occurrence unless business logic requires specific handling.

### 3. **Difference between `dropna()` and `fillna()`?**
**Answer**: 
- `dropna()` removes rows/columns with missing values
- `fillna()` replaces missing values with specified values (mean, median, mode, or custom)

### 4. **What is outlier treatment and why is it important?**
**Answer**: Outliers are extreme values that don't follow the data pattern. Treatment is important because outliers can skew statistical analyses and machine learning models. Common methods include capping, transformation, or removal.

### 5. **Explain the process of standardizing data**
**Answer**: Standardization involves making data consistent across the dataset:
- Text normalization (lowercase/uppercase)
- Category standardization (Male/M â†’ Male)
- Date format consistency
- Unit standardization

### 6. **How do you handle inconsistent data formats (e.g., dates)?**
**Answer**: Use `pd.to_datetime()` in Python with appropriate format specifications, or use Excel's "Format Cells" feature to standardize date formats.

### 7. **What are common data cleaning challenges?**
**Answer**: 
- Missing data handling decisions
- Duplicate identification and removal
- Inconsistent category labels
- Outlier detection and treatment
- Data type conversions
- Text cleaning and standardization

### 8. **How do you check data quality?**
**Answer**: 
- Summary statistics (`.describe()`)
- Missing value analysis (`.isnull().sum()`)
- Unique value counts (`.nunique()`)
- Data type validation (`.dtypes`)
- Data profiling and validation rules

---

## ðŸ’¡ Key Learnings
- **Hands-on practice** in data cleaning and preprocessing
- **Advanced Pandas** data manipulation techniques
- **Comprehensive data quality assessment** methods
- **Automated data cleaning pipeline** development
- **Statistical methods** for missing value imputation
- **Business logic implementation** for data validation

---

## ðŸ“ˆ Next Steps
The cleaned dataset is now ready for:
- **Exploratory Data Analysis (EDA)**
- **Customer Segmentation Analysis**
- **Predictive Modeling**
- **Business Intelligence Dashboards**

---

## ðŸ“„ License
This project is licensed under the MIT License - feel free to use, modify, and share.

---

## ðŸ‘¥ Author & Contributing

### **Author**
**Manish Sharma**  
Data Analyst Intern Candidate  
Elevate Labs Solutions  
manishsharma93155@gmail.com

### **Contributing**
1. Fork the project
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

---

## ðŸŽ¯ Interview Preparation
This project demonstrates practical skills in handling real-world data challenges, preparing you for technical interviews focusing on data cleaning, preprocessing, and quality assurance.

**ðŸš€ Ready for Analysis!** - The dataset is now clean, validated, and prepared for advanced analytical tasks.

---

## ðŸ“ž Contact
manishsharma93155@gmail.com
